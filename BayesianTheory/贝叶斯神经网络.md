### 贝叶斯神经网络

#### 前言：

一个神经网络模型可以视为一个条件分布模型$P(y|x,w)$,即在$x,w$已知的条件下求出$y$的分布，如果是分类问题，该分布对应分到各类的概率，如果是回归问题，则认为是高斯分布并取均值作为预测结果，相应地，神经网络的学习可以视作一个最大似然估计（Maximum Likelihood Estimation,MLE）
$$
w^{MLE}=\mathop{argmax}\limits_w \log P(D|w) \\
=\mathop{argmax}\limits_w \sum_i\log P(y_i|x_i,w)
$$
其中$D$对应训练所使用的数据集，回归问题中带入高斯分布可以得到平均平方误差，分类问题则带入逻辑函数可以推导出交叉熵，使用反向传播求解函数极值点。

在上一节区分贝叶斯估计和最大后验概率估计的过程中有提到两者的不同，在贝叶斯估计中，不考虑参数的先验分布，即认为取每个参数的机会是均等的；但在最大后验估计中，会认为参数的分布事先是服从某种分布的，如果将这种思想带入到上面，则对于$w$的估计就变成了求解最大后验概率（最大后验估计，Maximum Posteriori，MAP）：
$$
w^{MAP}=\mathop{argmax}\limits_w \log P(D|w)P(w)\\
=\mathop{argmax}\limits_w \log P(D|w)+ \log P(w)
$$

#### 贝叶斯的产生

贝叶斯估计同样引入先验假设，与最大后验估计不同的是，最大后验估计目的是求解使函数获得极值时的$w$，贝叶斯估计是求解后验分布$P(w|D)$

说的有点乱，直接说公式吧，根据原有的贝叶斯公式
$$
p(w|D)=\frac{p(D|w)p(w)}{p(D)}\\
posterior=\frac{likelihood*prior}{evidence}
$$
posterior:通过样本$D$​​得到参数$w$​​的概率，即后验概率

likelihood：通过参数$w$​​得到样本$D$​​的概率，即数据集的表现

prior：参数$\theta$的先验概率（类比抛硬币的例子）

evidence ：$p(D)=\int p(D|w)p(w)dw$​​，样本$D$​​​发生的概率，各种$w$​​条件下发生的概率的积分

**极大似然估计**：求解$p(D|w)$​的极大值

**极大后验估计**：求解$p(D|w)p(w)$​的极大值

**贝叶斯估计**：求解整个$\frac{p(D|w)p(w)}{p(D)}$​​，即$p(w|D)$​​的分布

> **共轭分布**：对于一个特定的似然函数，如果选定一个先验概率分布，通过贝叶斯公式计算得到的后验概率与先验概率是同分布的，则认为该分布为共轭先验。

**共轭分布的意义：**因为后验分布和先验分布形式相近，只是参数有所不同，这意味着当我们获得新的观察数据时，可以直接通过参数更新，获得新的后验分布，而这次的后验分布在下次新数据到来时作为新的先验分布，这个过程可以避免在参数更新时引入大量的计算。

回到神经网络，求解后验分布的形式变为：
$$
P(w|D)=E_{P(w|D)}[P(\hat{y} |\hat{x}, w)]
$$
这个式子意味着：当根据$\hat{x}$和$w$预测$\hat{y}$​的时候，应该**通过后验概率得到的分布所对应的所有神经网络的模型求取期望**（这显然不太可能）

另外一方面，求解后验概率本身就是难解的

#### 变分推断

[参考](https://www.youtube.com/watch?v=HxQ94L8n0vU)

**后验概率的求解存在什么问题？**

边缘概率$p(D)=\int p(D|w)p(w)dw$​的求解需要计算所有可能的模型，这在实际场景中是难以实现的

**如何解决？**

实际的后验概率总是一个复杂的函数，但是可以通过无数个高斯分布的模型来无限接近，变分推断的目的是，不直接计算后验概率$P(w|D)$,而是通过计算$q(w)\approx P(w|D)$近似求解，即假设$w$服从某种分布，这种分布一般为高斯分布，我们想办法找到这个分布是什么来让它近似的等于我们要求解的分布$P(w|D)$

在数学中，需要一个损失函数来判断近似的分布对于真实分布的拟合程度，在变分推断中，我们用的是**KL散度**，即**衡量两个分布之间的距离**，优化目标变成：
$$
q^*(w)=\mathop{\arg min} \limits_{q(w)\in Q}(KL(q(w)||P(w|D)))
$$
其中：
$$
KL(q(w)||P(w|D))=E_{w\in q(w)}[\log \frac{q(w)}{P(w|D)}]\\
=\int q(w)\log \frac{q(w)}{P(w|D)}dw
$$
我们没有先验概率，只有联合概率$P(w,D)$，根据贝叶斯公式
$$
P(D|w)=\frac{P(w,D)}{P(w)}
$$

> **联合概率的定义：两个条件同时满足的概率**，所以这样联合起来看$p(w,D)=p(D|w)p(w)$​​​​,​​即考虑所有可能的$w$​​情况下,$D$​​​的分布与实际分布一致的概率分别是多少？
>
> 注意这里与边缘概率$p(D)=\int p(D|w)p(w)dw$做区分，$p(D)$需要遍历所有可能的$w$，而$p(w,D)=p(D|w)p(w)$只针对要求解的特定的$w$

使用联合概率带入KL散度的公式中，得到：
$$
KL(q(w)||P(w|D))=\int q(w)\log \frac{q(w)}{P(w|D)}dw\\
=\int q(w)\log \frac{q(w)P(D)}{P(w,D)}dw\\
=\int q(w)\log \frac{q(w)}{P(w,D)}dw + \int q(w)\log P(D)dw\\
=E_{w\sim q(w)}[\log \frac{q(w)}{P(w,D)}]+E_{w\sim q(w)}[\log P(D)]\\
=-E_{w\sim q(w)}[\log \frac{P(w,D)}{q(w)}] + \log P(D)
$$
此时，我们令$\mathcal{L}(q)=E_{w\sim q(w)}[\log \frac{P(w,D)}{q(w)}]$​​，而$P(D)$​​通过观察数据集得到的，所以不会发生改变，即$P(D)$​​为固定值，且范围在0-1之间，取对数之后$\log P(D)$​​始终为负数，而KL散度的值必须为正数，如果要保证上式成立，则$E_{w\sim q(w)}[\log \frac{P(w,D)}{q(w)}]$​是一个负数，实际中KL散度不可能为 零，因此$E_{w\sim q(w)}[\log \frac{P(w,D)}{q(w)}]\leq \log P(D)$​，所以我们称之为**Evidence Lower Bound（ELBO）**​

这意味着如果我们找到了满足条件的$w$，使得$\mathcal{L}(w)=\log P(D)$,就相当于找到了后验概率，而实际中很难实现，所以一般是：
$$
q^*(w)=\mathop{\arg max} \limits_{q(w)\in Q}(\mathcal L(q))
$$
原来的KL散度可以重新写为$KL=-\mathcal L(q) + \log P(D)$​，重新排列顺序变成：
$$
\mathcal L(q)=-KL + \log P(D)
$$

#### 如何求解ELBO的上界？

==其实这里最难理解的就是各种参数倒来倒去==

有的式子中将$q(w)$写为$q(w|\theta)$，其实两者的含义是一样的，都表示某个特定的高斯分布，横坐标对应$w$的值，纵坐标对应取值的概率密度；$q(w|\theta)$的表示更具体一点，即$\theta$是常数空间中的一组常数$(\mu,\sigma)$​，表示一个特定的高斯的均值和方差取值，这样表示的意义是**在所有的高斯分布中取一个分布来进行拟合**。

最大化下界ELBO,即最大化
$$
\mathcal L=\sum_i\log q(w)-\sum_i \log P(w)-\sum_j \log P(y_j|w,x_j)
$$
其中$D=\{(x,y)\}$,为了求解极值，需要对期望进行求导，具体的算法之后再写

#### BNN实践

1. 从$N(\mu,\log (1+e^\rho))$​中采样，获得初始的$w$，这一步骤的目的是选取一个正态分布的参数
2. 分别计算$\log q(w)$​​,$\log P(w)$​​,$\log P(y_j|w,x_j)$​​,其中，计算$\log P(y_j|w,x_j)$​​实际上计算的是$\log P(y|y_{pred}),y_{pred}=w*x$​​,   $y$为真实值，也就可以得到$\mathcal L$​
3. 使用梯度下降法或其他方式更新参数

**python程序**

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Normal
import numpy as np
from scipy.stats import norm
import matplotlib.pyplot as plt

class Linear_BBB(nn.Module):
    """
        Layer of our BNN.
    """
    def __init__(self, input_features, output_features, prior_var=1.):
        """
            Initialization of our layer : our prior is a normal distribution
            centered in 0 and of variance 20.
        """
        # initialize layers
        super().__init__()
        # set input and output dimensions
        self.input_features = input_features
        self.output_features = output_features

        # initialize mu and rho parameters for the weights of the layer
        self.w_mu = nn.Parameter(torch.zeros(output_features, input_features))
        self.w_rho = nn.Parameter(torch.zeros(output_features, input_features))

        #initialize mu and rho parameters for the layer's bias
        self.b_mu =  nn.Parameter(torch.zeros(output_features))
        self.b_rho = nn.Parameter(torch.zeros(output_features))        

        #initialize weight samples (these will be calculated whenever the layer makes a prediction)
        self.w = None
        self.b = None

        # initialize prior distribution for all of the weights and biases
        self.prior = torch.distributions.Normal(0,prior_var)

    def forward(self, input):
        """
          Optimization process
        """
        # sample weights
        w_epsilon = Normal(0,1).sample(self.w_mu.shape)
        self.w = self.w_mu + torch.log(1+torch.exp(self.w_rho)) * w_epsilon

        # sample bias
        b_epsilon = Normal(0,1).sample(self.b_mu.shape)
        self.b = self.b_mu + torch.log(1+torch.exp(self.b_rho)) * b_epsilon

        # record log prior by evaluating log pdf of prior at sampled weight and bias
        w_log_prior = self.prior.log_prob(self.w)
        b_log_prior = self.prior.log_prob(self.b)
        self.log_prior = torch.sum(w_log_prior) + torch.sum(b_log_prior)

        # record log variational posterior by evaluating log pdf of normal distribution defined by parameters with respect at the sampled values
        self.w_post = Normal(self.w_mu.data, torch.log(1+torch.exp(self.w_rho)))
        self.b_post = Normal(self.b_mu.data, torch.log(1+torch.exp(self.b_rho)))
        self.log_post = self.w_post.log_prob(self.w).sum() + self.b_post.log_prob(self.b).sum()

        return F.linear(input, self.w, self.b)

class MLP_BBB(nn.Module):
    def __init__(self, hidden_units, noise_tol=.1,  prior_var=1.):

        # initialize the network like you would with a standard multilayer perceptron, but using the BBB layer
        super().__init__()
        self.hidden = Linear_BBB(1,hidden_units, prior_var=prior_var)
        self.out = Linear_BBB(hidden_units, 1, prior_var=prior_var)
        self.noise_tol = noise_tol # we will use the noise tolerance to calculate our likelihood

    def forward(self, x):
        # again, this is equivalent to a standard multilayer perceptron
        x = torch.sigmoid(self.hidden(x))
        x = self.out(x)
        return x

    def log_prior(self):
        # calculate the log prior over all the layers
        return self.hidden.log_prior + self.out.log_prior

    def log_post(self):
        # calculate the log posterior over all the layers
        return self.hidden.log_post + self.out.log_post

    def sample_elbo(self, input, target, samples):
        # we calculate the negative elbo, which will be our loss function
        #initialize tensors
        outputs = torch.zeros(samples, target.shape[0])
        log_priors = torch.zeros(samples)
        log_posts = torch.zeros(samples)
        log_likes = torch.zeros(samples)
        # make predictions and calculate prior, posterior, and likelihood for a given number of samples
        for i in range(samples):
            outputs[i] = self(input).reshape(-1) # make predictions
            log_priors[i] = self.log_prior() # get log prior
            log_posts[i] = self.log_post() # get log variational posterior
            log_likes[i] = Normal(outputs[i], self.noise_tol).log_prob(target.reshape(-1)).sum() # calculate the log likelihood
        # calculate monte carlo estimate of prior posterior and likelihood
        log_prior = log_priors.mean()
        log_post = log_posts.mean()
        log_like = log_likes.mean()
        # calculate the negative elbo (which is our loss function)
        loss = log_post - log_prior - log_like
        return loss

def toy_function(x):
    return -x**4 + 3*x**2 + 1

# toy dataset we can start with
x = torch.tensor([-2, -1.8, -1, 1, 1.8, 2]).reshape(-1,1)
y = toy_function(x)

net = MLP_BBB(32, prior_var=10)
optimizer = optim.Adam(net.parameters(), lr=.1)
epochs = 2000
for epoch in range(epochs):  # loop over the dataset multiple times
    optimizer.zero_grad()
    # forward + backward + optimize
    loss = net.sample_elbo(x, y, 1)
    loss.backward()
    optimizer.step()
    if epoch % 10 == 0:
        print('epoch: {}/{}'.format(epoch+1,epochs))
        print('Loss:', loss.item())
print('Finished Training')


# samples is the number of "predictions" we make for 1 x-value.
samples = 100
x_tmp = torch.linspace(-5,5,100).reshape(-1,1)
y_samp = np.zeros((samples,100))
for s in range(samples):
    y_tmp = net(x_tmp).detach().numpy()
    y_samp[s] = y_tmp.reshape(-1)
plt.plot(x_tmp.numpy(), np.mean(y_samp, axis = 0), label='Mean Posterior Predictive')
plt.fill_between(x_tmp.numpy().reshape(-1), np.percentile(y_samp, 2.5, axis = 0), np.percentile(y_samp, 97.5, axis = 0), alpha = 0.25, label='95% Confidence')
plt.legend()
plt.scatter(x, toy_function(x))
plt.title('Posterior Predictive')
plt.show()
```

